{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZ29daWzxRLL"
   },
   "source": [
    "# Building and Improving on LSTMs\n",
    "Lets build a long short term memory unit from scratch! For a clear and concise tutorial of LSTMs that heavily influenced this work, see [this writeup](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). The inner workings of LSTMs will also be explained throughout this notebook.\n",
    "\n",
    "Once we implement an a simple recurrent neural network (RNN) that makes use of LSTMs and test its performance, we will implement improvements based on the paper below titled \"Mogrifier LSTM\" and compare our results.\n",
    "\n",
    "\n",
    "## References\n",
    "[PyTorch LSTM Tutorial](https://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/)\n",
    "\n",
    "[Understanding LSTMs writeup](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[Video lesson on LSTMs from Andrew Ng](https://www.coursera.org/lecture/nlp-sequence-models/long-short-term-memory-lstm-KXoay)\n",
    "\n",
    "[Mogrifier LSTM](https://arxiv.org/abs/1909.01792)\n",
    "\n",
    "[LSTM Performance](https://github.com/sebastianruder/NLP-progress/blob/master/english/language_modeling.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHfykk4qlE0I"
   },
   "source": [
    "# RNNs\n",
    "In recurrent neural networks (RNNs), we generally want to maintain some memory of sequential data in order to make predictions. \n",
    ">![Image from wikipedia](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    ">*By François Deloche - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=60109157*\n",
    "\n",
    "In the above diagram, the main thing to keep in mind is how memory is maintained. Inputs $x_t$ feed into hidden states $h_t$. Each cell (one set of red blue and green shapes) in the RNN (1) receives information from some part in a sequence of data, (2) reads it into the hidden state, and (3) transforms that information and passes it on to the next hidden state in the chain. In this manner, past data is fed through the entire network in case it might be useful to some cell later in the chain.\n",
    "\n",
    "The problem with this approach to RNNs is that we generally run into the problem of vanishing gradients. What this means is that our data from past hidden states eventually gets multiplied to be so small as to be neglible. Our ability to remember things from even a few states ago is not so great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRmOAFDYlE0J"
   },
   "source": [
    "# Enter LSTMs\n",
    "To solve the problem of vanishing gradients, the idea of the long short term memory (LSTM) cell was created. This cell replaces the RNN cell referred to above in order to alleviate some issues with RNNs and acheive better performance on sequential data.\n",
    "\n",
    "## But how do LSTMs work?\n",
    "\n",
    "Essentially what LSTMs do is expand on the basic RNN cell by adding a parallel branch that tracks data that happened further in the past. In this way, we can avoid the vanishing gradient issue of basic RNN cells.\n",
    "The below diagram shows the flow of information in an LSTM cell.\n",
    "\n",
    ">![image](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1920px-The_LSTM_cell.png)\n",
    ">By Guillaume Chevalier - Own work, CC BY 4.0, https://commons.wikimedia.org/w/index.php?curid=71836793\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhLBgM7clE0J"
   },
   "source": [
    "The equations for the LSTM cell look like this (taken from [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):\n",
    "\n",
    "$\\begin{array}{ll} \\\\\n",
    "  f_t = \\sigma(W_{f} [x_t,h_{t-1}] + b_{f}) \\\\\n",
    "  i_t = \\sigma(W_{i} [x_t,h_{t-1}] + b_{i}) \\\\\n",
    "  \\tilde{C_t} = \\tanh(W_{ig} [x_t,h_{t-1}] + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "  o_t = \\sigma(W_{o} [x_t,h_{t-1}] + b_{o}) \\\\\n",
    "  C_t = f_t * C_{t-1} + i_t * \\tilde{C_t} \\\\\n",
    "  h_t = o_t * \\tanh(C_t) \\\\\n",
    "\\end{array}$\n",
    "\n",
    "*Note that $[x_t,h_{t-1}]$ refers to the concatenation of the $x_t$ and $h_{t-1}$ matrices. Some variations of LSTM implementations split these instead. Depending on the implementation, this may lead to some lost information.*\n",
    "\n",
    "$f_t$ is the function for the forget gate.\n",
    "\n",
    "$i_t$ is the function for the input gate.\n",
    "\n",
    "$\\tilde{C_t}$ is the function for the candidate memory cell update.\n",
    "\n",
    "$o_t$ is an intermediate calculation for determining the hidden state $h_t$.\n",
    "\n",
    "$C_t$ is the function for the updated memory cell.\n",
    "\n",
    "$h_t$ is the function for the update hidden state.\n",
    "\n",
    "**While initially daunting**, what is happening here is not too complicated. Essentially, we have two gates, one for determining whether we will forget ($f_t$) some part of our memory cell $C$ and an input/update gate for determining whether we will add new information into our memory cell ($i_t$). Finally, we update our hidden state $h$ based on whatever information is currently in the memory cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mdu2qsFlE0b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path(\"../data/brown\")\n",
    "N_EPOCHS = 210\n",
    "from enum import IntEnum\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGp-5a9PlE0a"
   },
   "source": [
    "# Implementing the LSTM\n",
    "\n",
    "Given what we've learned, lets implement our own LSTM in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if running the notebook from Google Colaboratory\n",
    "!pip install allennlp==0.8.0\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_c7huC6QlE0d",
    "outputId": "7307a3b2-938a-4557-a511-9718ed4e7561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        #Define/initialize all tensors   \n",
    "        # forget gate\n",
    "        self.Wf = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bf = Parameter(torch.Tensor(hidden_sz))\n",
    "        # input gate\n",
    "        self.Wi = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bi = Parameter(torch.Tensor(hidden_sz))\n",
    "        # Candidate memory cell\n",
    "        self.Wc = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bc = Parameter(torch.Tensor(hidden_sz))\n",
    "        # output gate\n",
    "        self.Wo = Parameter(torch.Tensor(input_sz+hidden_sz, hidden_sz))\n",
    "        self.bo = Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "        \n",
    "    #Define forward pass through all LSTM cells across all timesteps.\n",
    "    #By using PyTorch functions, we get backpropagation for free.\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                init_states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_sz, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        #ht and Ct start as the previous states and end as the output states in each loop bellow\n",
    "        if init_states is None:\n",
    "            ht = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "            Ct = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "        else:\n",
    "            ht, Ct = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            xt = x[:, t, :]\n",
    "            hx_concat = torch.cat((ht,xt),dim=1)\n",
    "\n",
    "            ### The LSTM Cell!\n",
    "            ft = torch.sigmoid(hx_concat @ self.Wf + self.bf)\n",
    "            it = torch.sigmoid(hx_concat @ self.Wi + self.bi)\n",
    "            Ct_candidate = torch.tanh(hx_concat @ self.Wc + self.bc)\n",
    "            ot = torch.sigmoid(hx_concat @ self.Wo + self.bo)\n",
    "            #outputs\n",
    "            Ct = ft * Ct + it * Ct_candidate\n",
    "            ht = ot * torch.tanh(Ct)\n",
    "            ###\n",
    "\n",
    "            hidden_seq.append(ht.unsqueeze(Dim.batch))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq, (ht, Ct)\n",
    "\n",
    "#sanity testing\n",
    "#note that our hidden_sz is also our defined output size for each LSTM cell.\n",
    "batch_sz, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n",
    "arr = torch.randn(batch_sz, seq_len, feat_sz)\n",
    "lstm = NaiveLSTM(feat_sz, hidden_sz)\n",
    "ht, (hn, cn) = lstm(arr)\n",
    "ht.shape #shape should be batch_sz x seq_len x hidden_sz = 5x10x16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSzGsmxplE0p"
   },
   "source": [
    "# Testing the Implementation\n",
    "Now, that we've covered the basics and have a minimally working LSTM, we'll put our model into action. Our testbed will be a character-level language modeling task. We'll be using the Brown Corpus which you can get via the commands below.\n",
    "\n",
    ">More information on the Brown corpus can be found [here](https://en.wikipedia.org/wiki/Brown_Corpus).\n",
    "\n",
    ">\"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "DTBnkZK1lE0q",
    "outputId": "fb0393f8-16fd-4881-b637-7df1555a834e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6040k  100 6040k    0     0  5687k      0  0:00:01  0:00:01 --:--:-- 5687k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {DATA_ROOT}\n",
    "!curl http://www.sls.hawaii.edu/bley-vroman/brown.txt -o {DATA_ROOT / \"brown.txt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCB38uxdlE0t"
   },
   "source": [
    "We'll let AllenNLP--an NLP library made to simplify training in PyTorch--handle the complexity of training the language model and building up the datasets. What's happening below is we are tokenizing the characters in the dataset and then splitting the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "UmCw9G_ulE0v",
    "outputId": "43f8a410-68e7-42cc-8323-16583c398122"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/11994 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 11994/11994 [00:00<00:00, 55196.20it/s][A\n",
      "11994it [00:07, 1706.61it/s]\n",
      "100%|██████████| 10794/10794 [00:03<00:00, 3145.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.dataset_readers import LanguageModelingReader\n",
    "from allennlp.data.tokenizers import CharacterTokenizer\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.training import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "char_tokenizer = CharacterTokenizer(lowercase_characters=True)\n",
    "\n",
    "reader = LanguageModelingReader(\n",
    "    tokens_per_instance=500,\n",
    "    tokenizer=char_tokenizer,\n",
    "    token_indexers = {\"tokens\": SingleIdTokenIndexer()},\n",
    ")\n",
    "\n",
    "train_ds = reader.read(DATA_ROOT / \"brown.txt\")\n",
    "train_ds, val_ds = train_test_split(train_ds, random_state=0, test_size=0.1)\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "\n",
    "iterator = BasicIterator(batch_size=32)\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MbtyvFIATNM"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, epochs: int,log_dir):\n",
    "    trainer = Trainer( patience=7,\n",
    "        histogram_interval=10,\n",
    "        summary_interval= 10,\n",
    "        serialization_dir=log_dir,\n",
    "        model=model.cuda() if torch.cuda.is_available() else model,\n",
    "        optimizer=optim.Adam(model.parameters()),\n",
    "        iterator=iterator, train_dataset=train_ds, \n",
    "        validation_dataset=val_ds, num_epochs=epochs,\n",
    "        cuda_device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB9eEFoT1GMN"
   },
   "source": [
    "We build our NLP neural network below using 3 layers:\n",
    "\n",
    "1.   An embedding layer\n",
    "2.   An encoding layer (a set of our LSTM cells based on the sequence size)\n",
    "3.   A projection layer (to convert the text back out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdVfTb4OlE0x"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.models import Model\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "\n",
    "class LanguageModel(Model):\n",
    "    def __init__(self, encoder: nn.RNN, vocab: Vocabulary,\n",
    "                 embedding_dim: int=50):\n",
    "        super().__init__(vocab=vocab)\n",
    "        # char embedding\n",
    "        self.vocab_size = vocab.get_vocab_size()\n",
    "        self.padding_idx = vocab.get_token_index(\"@@PADDING@@\")\n",
    "        token_embedding = Embedding(\n",
    "            num_embeddings=vocab.get_vocab_size(),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_index=self.padding_idx,\n",
    "        )\n",
    "        self.embedding = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.hidden_size, self.vocab_size)\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=self.padding_idx)\n",
    "    \n",
    "    def forward(self, input_tokens: Dict[str, torch.Tensor],\n",
    "                output_tokens: Dict[str, torch.Tensor]):\n",
    "        embs = self.embedding(input_tokens)\n",
    "        x, _ = self.encoder(embs)\n",
    "        x = self.projection(x)\n",
    "        if output_tokens is not None:\n",
    "            loss = self.loss(x.view((-1, self.vocab_size)), output_tokens[\"tokens\"].flatten())\n",
    "        else:\n",
    "            loss = None\n",
    "        return {\"loss\": loss, \"logits\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVO3DUOOlE0z"
   },
   "source": [
    "Now, let's try training. If you only want to verify this works, change N_EPOCHS to some small number (e.g. 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "stN7plj9lE0z",
    "outputId": "a98cbf1b-077f-4936-c763-15bcaba9b0aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.7291 ||: 100%|██████████| 338/338 [02:12<00:00,  2.54it/s]\n",
      "loss: 2.3597 ||: 100%|██████████| 38/38 [00:05<00:00,  7.49it/s]\n",
      "loss: 2.2287 ||: 100%|██████████| 338/338 [02:05<00:00,  2.70it/s]\n",
      "loss: 2.1205 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 2.0543 ||: 100%|██████████| 338/338 [02:05<00:00,  2.70it/s]\n",
      "loss: 1.9936 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.9513 ||: 100%|██████████| 338/338 [02:05<00:00,  2.70it/s]\n",
      "loss: 1.9082 ||: 100%|██████████| 38/38 [00:04<00:00,  8.59it/s]\n",
      "loss: 1.8780 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.8450 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.8224 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.7966 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.7784 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.7575 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.7425 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.7246 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.7127 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.6982 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.6869 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.6743 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.6646 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.6533 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.6456 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.6361 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.6283 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.6205 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.6132 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.6070 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.5999 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.5938 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.5878 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5832 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.5773 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5732 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.5673 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.5646 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.5587 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5566 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.5506 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5493 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.5432 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5425 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.5368 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5361 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.5306 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5302 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.5249 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5258 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.5196 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5214 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.5144 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5162 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.5100 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5119 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.5055 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.5084 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.5016 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5046 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.4978 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.5002 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.4940 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4986 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.4908 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4942 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4873 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4916 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.4843 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4891 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4813 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4860 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4788 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4836 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.4759 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4816 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.4732 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4788 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.4709 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4775 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.4685 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4753 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.4664 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4728 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4641 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4711 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4623 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4695 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4603 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4670 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4583 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4656 ||: 100%|██████████| 38/38 [00:04<00:00,  8.59it/s]\n",
      "loss: 1.4568 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4642 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.4548 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4624 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4532 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4608 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4515 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4603 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.4499 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4591 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4483 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4570 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4470 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4564 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4454 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4544 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4444 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4527 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4427 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4518 ||: 100%|██████████| 38/38 [00:04<00:00,  8.58it/s]\n",
      "loss: 1.4418 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4508 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.4404 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4497 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4392 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4492 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4380 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4484 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4369 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4472 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.4355 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4460 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4348 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4448 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4335 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4438 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4327 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4434 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.4317 ||: 100%|██████████| 338/338 [02:05<00:00,  2.70it/s]\n",
      "loss: 1.4434 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4306 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4414 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.4296 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4414 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4287 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4402 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.4280 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4389 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4271 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4386 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.4262 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4379 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4251 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4367 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4244 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4367 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.4237 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4355 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4230 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4347 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4220 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4339 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4212 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4344 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4207 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4328 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4199 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4325 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4191 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4317 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.4184 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4310 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.4177 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4310 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4169 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4305 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.4165 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4297 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.4158 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4287 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4152 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4286 ||: 100%|██████████| 38/38 [00:04<00:00,  8.76it/s]\n",
      "loss: 1.4146 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4282 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4142 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4276 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4133 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4275 ||: 100%|██████████| 38/38 [00:04<00:00,  8.75it/s]\n",
      "loss: 1.4128 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4273 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.4120 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4265 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4116 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4257 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.4110 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4255 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.4105 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4261 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4100 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4244 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.4095 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4235 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4088 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4239 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4084 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4230 ||: 100%|██████████| 38/38 [00:04<00:00,  8.76it/s]\n",
      "loss: 1.4081 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4228 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4074 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4223 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4072 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4220 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.4064 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4217 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4061 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4216 ||: 100%|██████████| 38/38 [00:04<00:00,  8.56it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 338/338 [02:05<00:00,  2.70it/s]\n",
      "loss: 1.4216 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.4051 ||: 100%|██████████| 338/338 [02:04<00:00,  2.70it/s]\n",
      "loss: 1.4203 ||: 100%|██████████| 38/38 [00:04<00:00,  8.60it/s]\n",
      "loss: 1.4050 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4204 ||: 100%|██████████| 38/38 [00:04<00:00,  8.75it/s]\n",
      "loss: 1.4042 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4193 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.4040 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4195 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4198 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4030 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4191 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.4024 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4188 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.4023 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4177 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.4015 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4182 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.4015 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4176 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.4009 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4170 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.4006 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4167 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.4002 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4170 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.3997 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4160 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3994 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4163 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3992 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4161 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3988 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4150 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3983 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4156 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3980 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4148 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3977 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4146 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3973 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4143 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3969 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4141 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3964 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4139 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.3964 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4133 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3958 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4134 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3955 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4132 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3952 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4131 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3950 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4130 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3946 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4129 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3944 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4126 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3940 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4120 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3938 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4125 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3935 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4110 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.3930 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4115 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3928 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4114 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.3925 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4109 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3923 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4110 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.3920 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4106 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3920 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4104 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3915 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4102 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3913 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4102 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.3910 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4104 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3908 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4096 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3905 ||: 100%|██████████| 338/338 [02:08<00:00,  2.63it/s]\n",
      "loss: 1.4104 ||: 100%|██████████| 38/38 [00:04<00:00,  8.40it/s]\n",
      "loss: 1.3899 ||: 100%|██████████| 338/338 [02:08<00:00,  2.64it/s]\n",
      "loss: 1.4095 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3898 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4091 ||: 100%|██████████| 38/38 [00:04<00:00,  8.60it/s]\n",
      "loss: 1.3899 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4091 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3896 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4086 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.3893 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4083 ||: 100%|██████████| 38/38 [00:04<00:00,  8.61it/s]\n",
      "loss: 1.3889 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4083 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3887 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4087 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3886 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4078 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.3880 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4081 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3881 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4081 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.3876 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4081 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.3875 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4072 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.3872 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4067 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3870 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4069 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3868 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4073 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.3866 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4071 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3864 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4068 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.3861 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4062 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3859 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4058 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3857 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4065 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.3855 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3852 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4061 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.3850 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3848 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3847 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4052 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.3845 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4058 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.3841 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3841 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4049 ||: 100%|██████████| 38/38 [00:04<00:00,  8.58it/s]\n",
      "loss: 1.3840 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4052 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3836 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4044 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3834 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4045 ||: 100%|██████████| 38/38 [00:04<00:00,  8.65it/s]\n",
      "loss: 1.3835 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4042 ||: 100%|██████████| 38/38 [00:04<00:00,  8.59it/s]\n",
      "loss: 1.3834 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4041 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3831 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4040 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3830 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4039 ||: 100%|██████████| 38/38 [00:04<00:00,  8.59it/s]\n",
      "loss: 1.3826 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4033 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3824 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4038 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3823 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4039 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3820 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4041 ||: 100%|██████████| 38/38 [00:04<00:00,  8.73it/s]\n",
      "loss: 1.3818 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3817 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 38/38 [00:04<00:00,  8.75it/s]\n",
      "loss: 1.3814 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4033 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3812 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4025 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3811 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4028 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3810 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4026 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3806 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4024 ||: 100%|██████████| 38/38 [00:04<00:00,  8.63it/s]\n",
      "loss: 1.3808 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4033 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3802 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4018 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n",
      "loss: 1.3803 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4019 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3803 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4023 ||: 100%|██████████| 38/38 [00:04<00:00,  8.69it/s]\n",
      "loss: 1.3802 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4019 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3799 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4022 ||: 100%|██████████| 38/38 [00:04<00:00,  8.71it/s]\n",
      "loss: 1.3798 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4016 ||: 100%|██████████| 38/38 [00:04<00:00,  8.68it/s]\n",
      "loss: 1.3797 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4023 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3794 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4024 ||: 100%|██████████| 38/38 [00:04<00:00,  8.74it/s]\n",
      "loss: 1.3792 ||: 100%|██████████| 338/338 [02:04<00:00,  2.70it/s]\n",
      "loss: 1.4013 ||: 100%|██████████| 38/38 [00:04<00:00,  8.72it/s]\n",
      "loss: 1.3789 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4022 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3789 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4014 ||: 100%|██████████| 38/38 [00:04<00:00,  8.64it/s]\n",
      "loss: 1.3788 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4018 ||: 100%|██████████| 38/38 [00:04<00:00,  8.62it/s]\n",
      "loss: 1.3786 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4012 ||: 100%|██████████| 38/38 [00:04<00:00,  8.70it/s]\n",
      "loss: 1.3784 ||: 100%|██████████| 338/338 [02:04<00:00,  2.71it/s]\n",
      "loss: 1.4014 ||: 100%|██████████| 38/38 [00:04<00:00,  8.67it/s]\n",
      "loss: 1.3782 ||: 100%|██████████| 338/338 [02:04<00:00,  2.72it/s]\n",
      "loss: 1.4019 ||: 100%|██████████| 38/38 [00:04<00:00,  8.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 207,\n",
       " 'peak_cpu_memory_MB': 2750.236,\n",
       " 'peak_gpu_0_memory_MB': 970,\n",
       " 'training_duration': '7:31:32.654401',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 209,\n",
       " 'epoch': 209,\n",
       " 'training_loss': 1.3781935724280996,\n",
       " 'training_cpu_memory_MB': 2750.236,\n",
       " 'training_gpu_0_memory_MB': 952,\n",
       " 'validation_loss': 1.401858257619958,\n",
       " 'best_validation_loss': 1.4012448003417568}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_naive = LanguageModel(NaiveLSTM(50, 125), vocab)\n",
    "LSTM_trainer = train(lm_naive,N_EPOCHS,\"./run/lstm\")\n",
    "LSTM_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LR7A7BZglE03"
   },
   "source": [
    "Now, let's compare with the official LSTM. We'll do this one until it looks like the loss is no longer decreasing so we can see what kind of accuracy this model is capable of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "1ATRWUGHlE03",
    "outputId": "b4127467-ea9b-49ed-bb65-147fc74231c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.6807 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 2.3096 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 2.1862 ||: 100%|██████████| 338/338 [00:13<00:00, 25.53it/s]\n",
      "loss: 2.0887 ||: 100%|██████████| 38/38 [00:00<00:00, 51.24it/s]\n",
      "loss: 2.0264 ||: 100%|██████████| 338/338 [00:13<00:00, 25.56it/s]\n",
      "loss: 1.9687 ||: 100%|██████████| 38/38 [00:00<00:00, 51.09it/s]\n",
      "loss: 1.9279 ||: 100%|██████████| 338/338 [00:13<00:00, 24.90it/s]\n",
      "loss: 1.8885 ||: 100%|██████████| 38/38 [00:00<00:00, 51.02it/s]\n",
      "loss: 1.8581 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.8262 ||: 100%|██████████| 38/38 [00:00<00:00, 50.78it/s]\n",
      "loss: 1.8030 ||: 100%|██████████| 338/338 [00:13<00:00, 25.32it/s]\n",
      "loss: 1.7782 ||: 100%|██████████| 38/38 [00:00<00:00, 50.99it/s]\n",
      "loss: 1.7596 ||: 100%|██████████| 338/338 [00:13<00:00, 25.14it/s]\n",
      "loss: 1.7394 ||: 100%|██████████| 38/38 [00:00<00:00, 51.49it/s]\n",
      "loss: 1.7240 ||: 100%|██████████| 338/338 [00:13<00:00, 25.43it/s]\n",
      "loss: 1.7071 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.6942 ||: 100%|██████████| 338/338 [00:13<00:00, 25.54it/s]\n",
      "loss: 1.6804 ||: 100%|██████████| 38/38 [00:00<00:00, 51.02it/s]\n",
      "loss: 1.6691 ||: 100%|██████████| 338/338 [00:13<00:00, 25.47it/s]\n",
      "loss: 1.6571 ||: 100%|██████████| 38/38 [00:00<00:00, 50.46it/s]\n",
      "loss: 1.6476 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.6380 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.6295 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.6214 ||: 100%|██████████| 38/38 [00:00<00:00, 50.87it/s]\n",
      "loss: 1.6132 ||: 100%|██████████| 338/338 [00:13<00:00, 25.09it/s]\n",
      "loss: 1.6062 ||: 100%|██████████| 38/38 [00:00<00:00, 51.12it/s]\n",
      "loss: 1.5993 ||: 100%|██████████| 338/338 [00:13<00:00, 25.11it/s]\n",
      "loss: 1.5933 ||: 100%|██████████| 38/38 [00:00<00:00, 51.23it/s]\n",
      "loss: 1.5868 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.5816 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.5757 ||: 100%|██████████| 338/338 [00:13<00:00, 25.53it/s]\n",
      "loss: 1.5716 ||: 100%|██████████| 38/38 [00:00<00:00, 50.94it/s]\n",
      "loss: 1.5659 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.5621 ||: 100%|██████████| 38/38 [00:00<00:00, 51.33it/s]\n",
      "loss: 1.5564 ||: 100%|██████████| 338/338 [00:13<00:00, 25.30it/s]\n",
      "loss: 1.5530 ||: 100%|██████████| 38/38 [00:00<00:00, 51.02it/s]\n",
      "loss: 1.5480 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.5457 ||: 100%|██████████| 38/38 [00:00<00:00, 51.31it/s]\n",
      "loss: 1.5405 ||: 100%|██████████| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.5386 ||: 100%|██████████| 38/38 [00:00<00:00, 50.91it/s]\n",
      "loss: 1.5337 ||: 100%|██████████| 338/338 [00:13<00:00, 25.24it/s]\n",
      "loss: 1.5327 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.5274 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.5260 ||: 100%|██████████| 38/38 [00:00<00:00, 51.23it/s]\n",
      "loss: 1.5214 ||: 100%|██████████| 338/338 [00:13<00:00, 25.47it/s]\n",
      "loss: 1.5214 ||: 100%|██████████| 38/38 [00:00<00:00, 51.09it/s]\n",
      "loss: 1.5159 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.5165 ||: 100%|██████████| 38/38 [00:00<00:00, 51.15it/s]\n",
      "loss: 1.5113 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.5109 ||: 100%|██████████| 38/38 [00:00<00:00, 51.03it/s]\n",
      "loss: 1.5066 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.5087 ||: 100%|██████████| 38/38 [00:00<00:00, 51.14it/s]\n",
      "loss: 1.5025 ||: 100%|██████████| 338/338 [00:13<00:00, 25.10it/s]\n",
      "loss: 1.5036 ||: 100%|██████████| 38/38 [00:00<00:00, 51.14it/s]\n",
      "loss: 1.4982 ||: 100%|██████████| 338/338 [00:13<00:00, 25.06it/s]\n",
      "loss: 1.4996 ||: 100%|██████████| 38/38 [00:00<00:00, 51.21it/s]\n",
      "loss: 1.4944 ||: 100%|██████████| 338/338 [00:13<00:00, 25.54it/s]\n",
      "loss: 1.4961 ||: 100%|██████████| 38/38 [00:00<00:00, 50.86it/s]\n",
      "loss: 1.4910 ||: 100%|██████████| 338/338 [00:13<00:00, 25.27it/s]\n",
      "loss: 1.4946 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 1.4874 ||: 100%|██████████| 338/338 [00:13<00:00, 25.59it/s]\n",
      "loss: 1.4907 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n",
      "loss: 1.4846 ||: 100%|██████████| 338/338 [00:13<00:00, 25.30it/s]\n",
      "loss: 1.4873 ||: 100%|██████████| 38/38 [00:00<00:00, 50.71it/s]\n",
      "loss: 1.4812 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4850 ||: 100%|██████████| 38/38 [00:00<00:00, 50.81it/s]\n",
      "loss: 1.4785 ||: 100%|██████████| 338/338 [00:13<00:00, 25.09it/s]\n",
      "loss: 1.4814 ||: 100%|██████████| 38/38 [00:00<00:00, 51.03it/s]\n",
      "loss: 1.4757 ||: 100%|██████████| 338/338 [00:13<00:00, 25.12it/s]\n",
      "loss: 1.4787 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 1.4732 ||: 100%|██████████| 338/338 [00:13<00:00, 25.47it/s]\n",
      "loss: 1.4777 ||: 100%|██████████| 38/38 [00:00<00:00, 50.98it/s]\n",
      "loss: 1.4707 ||: 100%|██████████| 338/338 [00:13<00:00, 25.53it/s]\n",
      "loss: 1.4750 ||: 100%|██████████| 38/38 [00:00<00:00, 51.12it/s]\n",
      "loss: 1.4684 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4726 ||: 100%|██████████| 38/38 [00:00<00:00, 51.04it/s]\n",
      "loss: 1.4660 ||: 100%|██████████| 338/338 [00:13<00:00, 25.33it/s]\n",
      "loss: 1.4709 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n",
      "loss: 1.4638 ||: 100%|██████████| 338/338 [00:13<00:00, 25.30it/s]\n",
      "loss: 1.4688 ||: 100%|██████████| 38/38 [00:00<00:00, 50.85it/s]\n",
      "loss: 1.4621 ||: 100%|██████████| 338/338 [00:13<00:00, 25.20it/s]\n",
      "loss: 1.4677 ||: 100%|██████████| 38/38 [00:00<00:00, 51.02it/s]\n",
      "loss: 1.4596 ||: 100%|██████████| 338/338 [00:13<00:00, 25.02it/s]\n",
      "loss: 1.4646 ||: 100%|██████████| 38/38 [00:00<00:00, 50.83it/s]\n",
      "loss: 1.4579 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4641 ||: 100%|██████████| 38/38 [00:00<00:00, 51.18it/s]\n",
      "loss: 1.4559 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4615 ||: 100%|██████████| 38/38 [00:00<00:00, 51.18it/s]\n",
      "loss: 1.4540 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4596 ||: 100%|██████████| 38/38 [00:00<00:00, 50.89it/s]\n",
      "loss: 1.4524 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4587 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4510 ||: 100%|██████████| 338/338 [00:13<00:00, 25.33it/s]\n",
      "loss: 1.4577 ||: 100%|██████████| 38/38 [00:00<00:00, 51.06it/s]\n",
      "loss: 1.4492 ||: 100%|██████████| 338/338 [00:13<00:00, 25.25it/s]\n",
      "loss: 1.4554 ||: 100%|██████████| 38/38 [00:00<00:00, 50.91it/s]\n",
      "loss: 1.4475 ||: 100%|██████████| 338/338 [00:13<00:00, 25.09it/s]\n",
      "loss: 1.4549 ||: 100%|██████████| 38/38 [00:00<00:00, 50.94it/s]\n",
      "loss: 1.4461 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4533 ||: 100%|██████████| 38/38 [00:00<00:00, 51.07it/s]\n",
      "loss: 1.4446 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4520 ||: 100%|██████████| 38/38 [00:00<00:00, 51.23it/s]\n",
      "loss: 1.4432 ||: 100%|██████████| 338/338 [00:13<00:00, 25.33it/s]\n",
      "loss: 1.4509 ||: 100%|██████████| 38/38 [00:00<00:00, 51.01it/s]\n",
      "loss: 1.4418 ||: 100%|██████████| 338/338 [00:13<00:00, 25.33it/s]\n",
      "loss: 1.4494 ||: 100%|██████████| 38/38 [00:00<00:00, 50.62it/s]\n",
      "loss: 1.4405 ||: 100%|██████████| 338/338 [00:13<00:00, 25.38it/s]\n",
      "loss: 1.4485 ||: 100%|██████████| 38/38 [00:00<00:00, 51.13it/s]\n",
      "loss: 1.4395 ||: 100%|██████████| 338/338 [00:13<00:00, 25.07it/s]\n",
      "loss: 1.4468 ||: 100%|██████████| 38/38 [00:00<00:00, 51.32it/s]\n",
      "loss: 1.4382 ||: 100%|██████████| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.4458 ||: 100%|██████████| 38/38 [00:00<00:00, 50.90it/s]\n",
      "loss: 1.4373 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4449 ||: 100%|██████████| 38/38 [00:00<00:00, 50.91it/s]\n",
      "loss: 1.4360 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4442 ||: 100%|██████████| 38/38 [00:00<00:00, 50.85it/s]\n",
      "loss: 1.4346 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4429 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.4338 ||: 100%|██████████| 338/338 [00:13<00:00, 25.24it/s]\n",
      "loss: 1.4423 ||: 100%|██████████| 38/38 [00:00<00:00, 50.99it/s]\n",
      "loss: 1.4326 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4411 ||: 100%|██████████| 38/38 [00:00<00:00, 51.38it/s]\n",
      "loss: 1.4315 ||: 100%|██████████| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.4405 ||: 100%|██████████| 38/38 [00:00<00:00, 51.17it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4306 ||: 100%|██████████| 338/338 [00:13<00:00, 25.02it/s]\n",
      "loss: 1.4393 ||: 100%|██████████| 38/38 [00:00<00:00, 51.41it/s]\n",
      "loss: 1.4295 ||: 100%|██████████| 338/338 [00:13<00:00, 25.47it/s]\n",
      "loss: 1.4385 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.4286 ||: 100%|██████████| 338/338 [00:13<00:00, 25.48it/s]\n",
      "loss: 1.4378 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4277 ||: 100%|██████████| 338/338 [00:13<00:00, 25.59it/s]\n",
      "loss: 1.4377 ||: 100%|██████████| 38/38 [00:00<00:00, 51.18it/s]\n",
      "loss: 1.4269 ||: 100%|██████████| 338/338 [00:13<00:00, 25.36it/s]\n",
      "loss: 1.4359 ||: 100%|██████████| 38/38 [00:00<00:00, 51.48it/s]\n",
      "loss: 1.4260 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4358 ||: 100%|██████████| 38/38 [00:00<00:00, 51.17it/s]\n",
      "loss: 1.4251 ||: 100%|██████████| 338/338 [00:13<00:00, 25.17it/s]\n",
      "loss: 1.4348 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n",
      "loss: 1.4244 ||: 100%|██████████| 338/338 [00:13<00:00, 25.09it/s]\n",
      "loss: 1.4343 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.4235 ||: 100%|██████████| 338/338 [00:13<00:00, 25.53it/s]\n",
      "loss: 1.4335 ||: 100%|██████████| 38/38 [00:00<00:00, 51.65it/s]\n",
      "loss: 1.4225 ||: 100%|██████████| 338/338 [00:13<00:00, 25.47it/s]\n",
      "loss: 1.4330 ||: 100%|██████████| 38/38 [00:00<00:00, 51.04it/s]\n",
      "loss: 1.4220 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4319 ||: 100%|██████████| 38/38 [00:00<00:00, 51.10it/s]\n",
      "loss: 1.4211 ||: 100%|██████████| 338/338 [00:13<00:00, 25.27it/s]\n",
      "loss: 1.4317 ||: 100%|██████████| 38/38 [00:00<00:00, 51.48it/s]\n",
      "loss: 1.4206 ||: 100%|██████████| 338/338 [00:14<00:00, 24.05it/s]\n",
      "loss: 1.4314 ||: 100%|██████████| 38/38 [00:00<00:00, 50.33it/s]\n",
      "loss: 1.4196 ||: 100%|██████████| 338/338 [00:15<00:00, 22.15it/s]\n",
      "loss: 1.4305 ||: 100%|██████████| 38/38 [00:00<00:00, 50.05it/s]\n",
      "loss: 1.4191 ||: 100%|██████████| 338/338 [00:15<00:00, 21.44it/s]\n",
      "loss: 1.4303 ||: 100%|██████████| 38/38 [00:00<00:00, 50.13it/s]\n",
      "loss: 1.4182 ||: 100%|██████████| 338/338 [00:13<00:00, 25.24it/s]\n",
      "loss: 1.4291 ||: 100%|██████████| 38/38 [00:00<00:00, 51.24it/s]\n",
      "loss: 1.4176 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.4293 ||: 100%|██████████| 38/38 [00:00<00:00, 51.66it/s]\n",
      "loss: 1.4169 ||: 100%|██████████| 338/338 [00:13<00:00, 25.53it/s]\n",
      "loss: 1.4277 ||: 100%|██████████| 38/38 [00:00<00:00, 51.42it/s]\n",
      "loss: 1.4162 ||: 100%|██████████| 338/338 [00:13<00:00, 25.43it/s]\n",
      "loss: 1.4278 ||: 100%|██████████| 38/38 [00:00<00:00, 51.26it/s]\n",
      "loss: 1.4156 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4271 ||: 100%|██████████| 38/38 [00:00<00:00, 51.29it/s]\n",
      "loss: 1.4149 ||: 100%|██████████| 338/338 [00:13<00:00, 25.24it/s]\n",
      "loss: 1.4267 ||: 100%|██████████| 38/38 [00:00<00:00, 51.39it/s]\n",
      "loss: 1.4143 ||: 100%|██████████| 338/338 [00:13<00:00, 25.08it/s]\n",
      "loss: 1.4265 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4138 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4261 ||: 100%|██████████| 38/38 [00:00<00:00, 51.22it/s]\n",
      "loss: 1.4130 ||: 100%|██████████| 338/338 [00:13<00:00, 25.65it/s]\n",
      "loss: 1.4251 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n",
      "loss: 1.4123 ||: 100%|██████████| 338/338 [00:13<00:00, 25.52it/s]\n",
      "loss: 1.4248 ||: 100%|██████████| 38/38 [00:00<00:00, 50.50it/s]\n",
      "loss: 1.4121 ||: 100%|██████████| 338/338 [00:14<00:00, 24.10it/s]\n",
      "loss: 1.4247 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4116 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4240 ||: 100%|██████████| 38/38 [00:00<00:00, 51.21it/s]\n",
      "loss: 1.4108 ||: 100%|██████████| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.4236 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4102 ||: 100%|██████████| 338/338 [00:13<00:00, 25.16it/s]\n",
      "loss: 1.4234 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.4097 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4225 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.4092 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4222 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.4088 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4219 ||: 100%|██████████| 38/38 [00:00<00:00, 51.47it/s]\n",
      "loss: 1.4082 ||: 100%|██████████| 338/338 [00:13<00:00, 25.38it/s]\n",
      "loss: 1.4210 ||: 100%|██████████| 38/38 [00:00<00:00, 50.81it/s]\n",
      "loss: 1.4077 ||: 100%|██████████| 338/338 [00:13<00:00, 25.36it/s]\n",
      "loss: 1.4217 ||: 100%|██████████| 38/38 [00:00<00:00, 51.23it/s]\n",
      "loss: 1.4072 ||: 100%|██████████| 338/338 [00:13<00:00, 25.14it/s]\n",
      "loss: 1.4204 ||: 100%|██████████| 38/38 [00:00<00:00, 51.13it/s]\n",
      "loss: 1.4066 ||: 100%|██████████| 338/338 [00:14<00:00, 23.65it/s]\n",
      "loss: 1.4201 ||: 100%|██████████| 38/38 [00:00<00:00, 51.35it/s]\n",
      "loss: 1.4061 ||: 100%|██████████| 338/338 [00:13<00:00, 25.48it/s]\n",
      "loss: 1.4200 ||: 100%|██████████| 38/38 [00:00<00:00, 51.18it/s]\n",
      "loss: 1.4059 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4201 ||: 100%|██████████| 38/38 [00:00<00:00, 51.17it/s]\n",
      "loss: 1.4054 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4188 ||: 100%|██████████| 38/38 [00:00<00:00, 51.14it/s]\n",
      "loss: 1.4050 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4189 ||: 100%|██████████| 38/38 [00:00<00:00, 51.52it/s]\n",
      "loss: 1.4046 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4191 ||: 100%|██████████| 38/38 [00:00<00:00, 51.39it/s]\n",
      "loss: 1.4039 ||: 100%|██████████| 338/338 [00:13<00:00, 25.15it/s]\n",
      "loss: 1.4178 ||: 100%|██████████| 38/38 [00:00<00:00, 51.02it/s]\n",
      "loss: 1.4036 ||: 100%|██████████| 338/338 [00:13<00:00, 25.02it/s]\n",
      "loss: 1.4186 ||: 100%|██████████| 38/38 [00:00<00:00, 51.33it/s]\n",
      "loss: 1.4032 ||: 100%|██████████| 338/338 [00:13<00:00, 25.52it/s]\n",
      "loss: 1.4178 ||: 100%|██████████| 38/38 [00:00<00:00, 51.33it/s]\n",
      "loss: 1.4029 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4175 ||: 100%|██████████| 38/38 [00:00<00:00, 51.37it/s]\n",
      "loss: 1.4023 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4168 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4022 ||: 100%|██████████| 338/338 [00:13<00:00, 25.26it/s]\n",
      "loss: 1.4166 ||: 100%|██████████| 38/38 [00:00<00:00, 51.28it/s]\n",
      "loss: 1.4015 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4158 ||: 100%|██████████| 38/38 [00:00<00:00, 50.91it/s]\n",
      "loss: 1.4010 ||: 100%|██████████| 338/338 [00:13<00:00, 25.13it/s]\n",
      "loss: 1.4170 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.4009 ||: 100%|██████████| 338/338 [00:13<00:00, 25.08it/s]\n",
      "loss: 1.4157 ||: 100%|██████████| 38/38 [00:00<00:00, 51.16it/s]\n",
      "loss: 1.4002 ||: 100%|██████████| 338/338 [00:13<00:00, 25.38it/s]\n",
      "loss: 1.4162 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.4000 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4148 ||: 100%|██████████| 38/38 [00:00<00:00, 50.42it/s]\n",
      "loss: 1.3996 ||: 100%|██████████| 338/338 [00:13<00:00, 25.35it/s]\n",
      "loss: 1.4151 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.3994 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4150 ||: 100%|██████████| 38/38 [00:00<00:00, 50.91it/s]\n",
      "loss: 1.3990 ||: 100%|██████████| 338/338 [00:13<00:00, 25.52it/s]\n",
      "loss: 1.4152 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.3984 ||: 100%|██████████| 338/338 [00:13<00:00, 25.22it/s]\n",
      "loss: 1.4139 ||: 100%|██████████| 38/38 [00:00<00:00, 51.24it/s]\n",
      "loss: 1.3982 ||: 100%|██████████| 338/338 [00:13<00:00, 25.08it/s]\n",
      "loss: 1.4143 ||: 100%|██████████| 38/38 [00:00<00:00, 51.21it/s]\n",
      "loss: 1.3980 ||: 100%|██████████| 338/338 [00:13<00:00, 25.38it/s]\n",
      "loss: 1.4139 ||: 100%|██████████| 38/38 [00:00<00:00, 51.14it/s]\n",
      "loss: 1.3977 ||: 100%|██████████| 338/338 [00:13<00:00, 25.57it/s]\n",
      "loss: 1.4130 ||: 100%|██████████| 38/38 [00:00<00:00, 50.99it/s]\n",
      "loss: 1.3971 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4131 ||: 100%|██████████| 38/38 [00:00<00:00, 51.24it/s]\n",
      "loss: 1.3971 ||: 100%|██████████| 338/338 [00:13<00:00, 25.38it/s]\n",
      "loss: 1.4128 ||: 100%|██████████| 38/38 [00:00<00:00, 51.16it/s]\n",
      "loss: 1.3966 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4120 ||: 100%|██████████| 38/38 [00:00<00:00, 51.15it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3963 ||: 100%|██████████| 338/338 [00:13<00:00, 25.24it/s]\n",
      "loss: 1.4119 ||: 100%|██████████| 38/38 [00:00<00:00, 50.97it/s]\n",
      "loss: 1.3960 ||: 100%|██████████| 338/338 [00:13<00:00, 25.01it/s]\n",
      "loss: 1.4117 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.3958 ||: 100%|██████████| 338/338 [00:13<00:00, 25.58it/s]\n",
      "loss: 1.4124 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.3951 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4110 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3949 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4121 ||: 100%|██████████| 38/38 [00:00<00:00, 51.13it/s]\n",
      "loss: 1.3946 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4115 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3945 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4107 ||: 100%|██████████| 38/38 [00:00<00:00, 51.48it/s]\n",
      "loss: 1.3941 ||: 100%|██████████| 338/338 [00:13<00:00, 25.17it/s]\n",
      "loss: 1.4111 ||: 100%|██████████| 38/38 [00:00<00:00, 50.97it/s]\n",
      "loss: 1.3938 ||: 100%|██████████| 338/338 [00:13<00:00, 25.05it/s]\n",
      "loss: 1.4109 ||: 100%|██████████| 38/38 [00:00<00:00, 51.41it/s]\n",
      "loss: 1.3933 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4098 ||: 100%|██████████| 38/38 [00:00<00:00, 50.97it/s]\n",
      "loss: 1.3930 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.4094 ||: 100%|██████████| 38/38 [00:00<00:00, 51.38it/s]\n",
      "loss: 1.3930 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4102 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.3927 ||: 100%|██████████| 338/338 [00:13<00:00, 25.32it/s]\n",
      "loss: 1.4103 ||: 100%|██████████| 38/38 [00:00<00:00, 51.22it/s]\n",
      "loss: 1.3926 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4088 ||: 100%|██████████| 38/38 [00:00<00:00, 51.01it/s]\n",
      "loss: 1.3922 ||: 100%|██████████| 338/338 [00:13<00:00, 25.18it/s]\n",
      "loss: 1.4089 ||: 100%|██████████| 38/38 [00:00<00:00, 51.59it/s]\n",
      "loss: 1.3920 ||: 100%|██████████| 338/338 [00:13<00:00, 25.06it/s]\n",
      "loss: 1.4095 ||: 100%|██████████| 38/38 [00:00<00:00, 51.33it/s]\n",
      "loss: 1.3916 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4087 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.3913 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.4094 ||: 100%|██████████| 38/38 [00:00<00:00, 51.04it/s]\n",
      "loss: 1.3909 ||: 100%|██████████| 338/338 [00:13<00:00, 25.48it/s]\n",
      "loss: 1.4091 ||: 100%|██████████| 38/38 [00:00<00:00, 51.49it/s]\n",
      "loss: 1.3906 ||: 100%|██████████| 338/338 [00:13<00:00, 25.33it/s]\n",
      "loss: 1.4084 ||: 100%|██████████| 38/38 [00:00<00:00, 50.94it/s]\n",
      "loss: 1.3906 ||: 100%|██████████| 338/338 [00:13<00:00, 25.40it/s]\n",
      "loss: 1.4083 ||: 100%|██████████| 38/38 [00:00<00:00, 50.95it/s]\n",
      "loss: 1.3903 ||: 100%|██████████| 338/338 [00:13<00:00, 25.26it/s]\n",
      "loss: 1.4081 ||: 100%|██████████| 38/38 [00:00<00:00, 50.97it/s]\n",
      "loss: 1.3900 ||: 100%|██████████| 338/338 [00:13<00:00, 25.09it/s]\n",
      "loss: 1.4073 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.3900 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4076 ||: 100%|██████████| 38/38 [00:00<00:00, 50.95it/s]\n",
      "loss: 1.3895 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4070 ||: 100%|██████████| 38/38 [00:00<00:00, 51.16it/s]\n",
      "loss: 1.3894 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4072 ||: 100%|██████████| 38/38 [00:00<00:00, 51.15it/s]\n",
      "loss: 1.3891 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4063 ||: 100%|██████████| 38/38 [00:00<00:00, 51.23it/s]\n",
      "loss: 1.3889 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.4068 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 1.3886 ||: 100%|██████████| 338/338 [00:13<00:00, 25.20it/s]\n",
      "loss: 1.4058 ||: 100%|██████████| 38/38 [00:00<00:00, 50.38it/s]\n",
      "loss: 1.3883 ||: 100%|██████████| 338/338 [00:13<00:00, 24.99it/s]\n",
      "loss: 1.4068 ||: 100%|██████████| 38/38 [00:00<00:00, 51.36it/s]\n",
      "loss: 1.3881 ||: 100%|██████████| 338/338 [00:13<00:00, 25.43it/s]\n",
      "loss: 1.4069 ||: 100%|██████████| 38/38 [00:00<00:00, 50.98it/s]\n",
      "loss: 1.3878 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4070 ||: 100%|██████████| 38/38 [00:00<00:00, 51.39it/s]\n",
      "loss: 1.3876 ||: 100%|██████████| 338/338 [00:13<00:00, 25.28it/s]\n",
      "loss: 1.4054 ||: 100%|██████████| 38/38 [00:00<00:00, 51.10it/s]\n",
      "loss: 1.3875 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4059 ||: 100%|██████████| 38/38 [00:00<00:00, 51.17it/s]\n",
      "loss: 1.3872 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4062 ||: 100%|██████████| 38/38 [00:00<00:00, 51.07it/s]\n",
      "loss: 1.3872 ||: 100%|██████████| 338/338 [00:13<00:00, 25.17it/s]\n",
      "loss: 1.4056 ||: 100%|██████████| 38/38 [00:00<00:00, 50.97it/s]\n",
      "loss: 1.3870 ||: 100%|██████████| 338/338 [00:13<00:00, 25.19it/s]\n",
      "loss: 1.4057 ||: 100%|██████████| 38/38 [00:00<00:00, 51.14it/s]\n",
      "loss: 1.3865 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4062 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3863 ||: 100%|██████████| 338/338 [00:13<00:00, 25.51it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 38/38 [00:00<00:00, 51.17it/s]\n",
      "loss: 1.3863 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4047 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.3860 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4054 ||: 100%|██████████| 38/38 [00:00<00:00, 51.34it/s]\n",
      "loss: 1.3855 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4046 ||: 100%|██████████| 38/38 [00:00<00:00, 51.12it/s]\n",
      "loss: 1.3854 ||: 100%|██████████| 338/338 [00:13<00:00, 25.19it/s]\n",
      "loss: 1.4046 ||: 100%|██████████| 38/38 [00:00<00:00, 51.18it/s]\n",
      "loss: 1.3854 ||: 100%|██████████| 338/338 [00:13<00:00, 25.01it/s]\n",
      "loss: 1.4050 ||: 100%|██████████| 38/38 [00:00<00:00, 51.08it/s]\n",
      "loss: 1.3851 ||: 100%|██████████| 338/338 [00:13<00:00, 25.34it/s]\n",
      "loss: 1.4040 ||: 100%|██████████| 38/38 [00:00<00:00, 51.24it/s]\n",
      "loss: 1.3847 ||: 100%|██████████| 338/338 [00:13<00:00, 25.37it/s]\n",
      "loss: 1.4042 ||: 100%|██████████| 38/38 [00:00<00:00, 50.53it/s]\n",
      "loss: 1.3847 ||: 100%|██████████| 338/338 [00:13<00:00, 25.55it/s]\n",
      "loss: 1.4040 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3844 ||: 100%|██████████| 338/338 [00:13<00:00, 25.29it/s]\n",
      "loss: 1.4040 ||: 100%|██████████| 38/38 [00:00<00:00, 51.16it/s]\n",
      "loss: 1.3843 ||: 100%|██████████| 338/338 [00:13<00:00, 25.42it/s]\n",
      "loss: 1.4036 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n",
      "loss: 1.3841 ||: 100%|██████████| 338/338 [00:13<00:00, 25.13it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 38/38 [00:00<00:00, 51.15it/s]\n",
      "loss: 1.3838 ||: 100%|██████████| 338/338 [00:13<00:00, 25.08it/s]\n",
      "loss: 1.4036 ||: 100%|██████████| 38/38 [00:00<00:00, 51.26it/s]\n",
      "loss: 1.3837 ||: 100%|██████████| 338/338 [00:13<00:00, 25.43it/s]\n",
      "loss: 1.4036 ||: 100%|██████████| 38/38 [00:00<00:00, 51.34it/s]\n",
      "loss: 1.3836 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4029 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 1.3834 ||: 100%|██████████| 338/338 [00:13<00:00, 25.36it/s]\n",
      "loss: 1.4030 ||: 100%|██████████| 38/38 [00:00<00:00, 50.90it/s]\n",
      "loss: 1.3830 ||: 100%|██████████| 338/338 [00:13<00:00, 25.27it/s]\n",
      "loss: 1.4026 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.3829 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4033 ||: 100%|██████████| 38/38 [00:00<00:00, 50.68it/s]\n",
      "loss: 1.3829 ||: 100%|██████████| 338/338 [00:13<00:00, 25.23it/s]\n",
      "loss: 1.4026 ||: 100%|██████████| 38/38 [00:00<00:00, 51.05it/s]\n",
      "loss: 1.3826 ||: 100%|██████████| 338/338 [00:13<00:00, 25.10it/s]\n",
      "loss: 1.4029 ||: 100%|██████████| 38/38 [00:00<00:00, 51.30it/s]\n",
      "loss: 1.3822 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4024 ||: 100%|██████████| 38/38 [00:00<00:00, 51.27it/s]\n",
      "loss: 1.3825 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4028 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3820 ||: 100%|██████████| 338/338 [00:13<00:00, 25.41it/s]\n",
      "loss: 1.4026 ||: 100%|██████████| 38/38 [00:00<00:00, 50.79it/s]\n",
      "loss: 1.3818 ||: 100%|██████████| 338/338 [00:13<00:00, 25.39it/s]\n",
      "loss: 1.4022 ||: 100%|██████████| 38/38 [00:00<00:00, 51.25it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3818 ||: 100%|██████████| 338/338 [00:13<00:00, 25.44it/s]\n",
      "loss: 1.4023 ||: 100%|██████████| 38/38 [00:00<00:00, 51.80it/s]\n",
      "loss: 1.3817 ||: 100%|██████████| 338/338 [00:13<00:00, 25.12it/s]\n",
      "loss: 1.4021 ||: 100%|██████████| 38/38 [00:00<00:00, 51.29it/s]\n",
      "loss: 1.3813 ||: 100%|██████████| 338/338 [00:13<00:00, 25.08it/s]\n",
      "loss: 1.4013 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3811 ||: 100%|██████████| 338/338 [00:13<00:00, 25.49it/s]\n",
      "loss: 1.4014 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3809 ||: 100%|██████████| 338/338 [00:13<00:00, 25.50it/s]\n",
      "loss: 1.4020 ||: 100%|██████████| 38/38 [00:00<00:00, 51.26it/s]\n",
      "loss: 1.3809 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4018 ||: 100%|██████████| 38/38 [00:00<00:00, 51.34it/s]\n",
      "loss: 1.3805 ||: 100%|██████████| 338/338 [00:13<00:00, 25.34it/s]\n",
      "loss: 1.4017 ||: 100%|██████████| 38/38 [00:00<00:00, 51.06it/s]\n",
      "loss: 1.3805 ||: 100%|██████████| 338/338 [00:13<00:00, 25.46it/s]\n",
      "loss: 1.4010 ||: 100%|██████████| 38/38 [00:00<00:00, 51.50it/s]\n",
      "loss: 1.3804 ||: 100%|██████████| 338/338 [00:13<00:00, 25.17it/s]\n",
      "loss: 1.4015 ||: 100%|██████████| 38/38 [00:00<00:00, 51.51it/s]\n",
      "loss: 1.3801 ||: 100%|██████████| 338/338 [00:13<00:00, 25.11it/s]\n",
      "loss: 1.4009 ||: 100%|██████████| 38/38 [00:00<00:00, 51.11it/s]\n",
      "loss: 1.3799 ||: 100%|██████████| 338/338 [00:13<00:00, 25.36it/s]\n",
      "loss: 1.4012 ||: 100%|██████████| 38/38 [00:00<00:00, 50.01it/s]\n",
      "loss: 1.3800 ||: 100%|██████████| 338/338 [00:13<00:00, 24.56it/s]\n",
      "loss: 1.4002 ||: 100%|██████████| 38/38 [00:00<00:00, 51.19it/s]\n",
      "loss: 1.3800 ||: 100%|██████████| 338/338 [00:13<00:00, 25.29it/s]\n",
      "loss: 1.4011 ||: 100%|██████████| 38/38 [00:00<00:00, 51.20it/s]\n",
      "loss: 1.3794 ||: 100%|██████████| 338/338 [00:13<00:00, 25.34it/s]\n",
      "loss: 1.4007 ||: 100%|██████████| 38/38 [00:00<00:00, 50.85it/s]\n",
      "loss: 1.3795 ||: 100%|██████████| 338/338 [00:13<00:00, 25.45it/s]\n",
      "loss: 1.4010 ||: 100%|██████████| 38/38 [00:00<00:00, 51.39it/s]\n",
      "loss: 1.3795 ||: 100%|██████████| 338/338 [00:13<00:00, 25.13it/s]\n",
      "loss: 1.4008 ||: 100%|██████████| 38/38 [00:00<00:00, 51.34it/s]\n",
      "loss: 1.3791 ||: 100%|██████████| 338/338 [00:13<00:00, 25.02it/s]\n",
      "loss: 1.4005 ||: 100%|██████████| 38/38 [00:00<00:00, 51.21it/s]\n",
      "loss: 1.3790 ||: 100%|██████████| 338/338 [00:13<00:00, 25.43it/s]\n",
      "loss: 1.4007 ||: 100%|██████████| 38/38 [00:00<00:00, 50.41it/s]\n",
      "loss: 1.3788 ||: 100%|██████████| 338/338 [00:13<00:00, 25.52it/s]\n",
      "loss: 1.4006 ||: 100%|██████████| 38/38 [00:00<00:00, 51.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 197,\n",
       " 'peak_cpu_memory_MB': 2751.964,\n",
       " 'peak_gpu_0_memory_MB': 1017,\n",
       " 'training_duration': '0:48:11.254805',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 203,\n",
       " 'epoch': 203,\n",
       " 'training_loss': 1.3790437656746815,\n",
       " 'training_cpu_memory_MB': 2751.964,\n",
       " 'training_gpu_0_memory_MB': 1014,\n",
       " 'validation_loss': 1.4007186230860258,\n",
       " 'best_validation_loss': 1.4002491518070823}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_comparison = LanguageModel(nn.LSTM(50, 125, batch_first=True), vocab)\n",
    "official_LSTM = train(lm_comparison, N_EPOCHS,\"./run/officiallstm\")\n",
    "official_LSTM.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TW0cc_YJdJqL"
   },
   "source": [
    "It looks like our basic implementation without any optimizations is a little over 100x slower than the built in one (185 seconds vs 15 seconds in my testing) while the number of iterations it takes to acheive similar accuracy is about the same. We could investigate making further improvements to ours (more efficient batches, fewer calculations, etc.), but lets leave it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDip4uF3lE06"
   },
   "source": [
    "# Mogrifier LSTM!\n",
    "\n",
    "Lets implement a version of the Mogrifier LSTM based on [this paper](https://arxiv.org/abs/1909.01792).\n",
    "\n",
    ">![mogrifier](https://drive.google.com/uc?id=1yMgPjXW_SV29Y-uuJsKNdknhm5CylWAu)\n",
    "\n",
    "Essentially what this paper does is provide another gate prior to the input into each LSTM cell that is entirely based on the interaction between the hidden state and the input. Read the paper if you'd like the intuition behind it.\n",
    "\n",
    "Lets look at the equations we need to implement:\n",
    "\n",
    "$x^i = 2\\sigma(Q^ih^{i-1}_{prev}) * x^{i-2}$ for odd $i \\in [1...r]$\n",
    "\n",
    "$h^i_{prev} = 2\\sigma(R^ix^{i-1}) * h^{i-2}_{prev}$ for even $i \\in [1...r]$\n",
    "\n",
    "So all we have to do is add some randomly initialized weights $Q$ and $R$ that will gate the inputs $x_t$ and $h_{t-1}$ in alternating fashion. Lets see how it compares!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TvlXNi22Jwkh",
    "outputId": "38d66788-9c8c-4a3a-be62-e97353d5a2b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MogLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int, mog_iterations: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.mog_iterations = mog_iterations\n",
    "        #Define/initialize all tensors   \n",
    "        self.Wih = Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.Whh = Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bih = Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.bhh = Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        #Mogrifiers\n",
    "        self.Q = Parameter(torch.Tensor(hidden_sz,input_sz))\n",
    "        self.R = Parameter(torch.Tensor(input_sz,hidden_sz))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "    def mogrify(self,xt,ht):\n",
    "      for i in range(1,self.mog_iterations+1):\n",
    "        if (i % 2 == 0):\n",
    "          ht = (2*torch.sigmoid(xt @ self.R)) * ht\n",
    "        else:\n",
    "          xt = (2*torch.sigmoid(ht @ self.Q)) * xt\n",
    "      return xt, ht\n",
    "\n",
    "    \n",
    "    #Define forward pass through all LSTM cells across all timesteps.\n",
    "    #By using PyTorch functions, we get backpropagation for free.\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                init_states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        batch_sz, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        #ht and Ct start as the previous states and end as the output states in each loop below\n",
    "        if init_states is None:\n",
    "            ht = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "            Ct = torch.zeros((batch_sz,self.hidden_size)).to(x.device)\n",
    "        else:\n",
    "            ht, Ct = init_states\n",
    "        for t in range(seq_sz): # iterate over the time steps\n",
    "            xt = x[:, t, :]\n",
    "            xt, ht = self.mogrify(xt,ht) #mogrification\n",
    "            gates = (xt @ self.Wih + self.bih) + (ht @ self.Whh + self.bhh)\n",
    "            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "            ### The LSTM Cell!\n",
    "            ft = torch.sigmoid(forgetgate)\n",
    "            it = torch.sigmoid(ingate)\n",
    "            Ct_candidate = torch.tanh(cellgate)\n",
    "            ot = torch.sigmoid(outgate)\n",
    "            #outputs\n",
    "            Ct = (ft * Ct) + (it * Ct_candidate)\n",
    "            ht = ot * torch.tanh(Ct)\n",
    "            ###\n",
    "\n",
    "            hidden_seq.append(ht.unsqueeze(Dim.batch))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=Dim.batch)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(Dim.batch, Dim.seq).contiguous()\n",
    "        return hidden_seq, (ht, Ct)\n",
    "\n",
    "#sanity testing\n",
    "#note that our hidden_sz is also our defined output size for each LSTM cell.\n",
    "batch_sz, seq_len, feat_sz, hidden_sz = 5, 10, 32, 16\n",
    "arr = torch.randn(batch_sz, seq_len, feat_sz)\n",
    "lstm = NaiveLSTM(feat_sz, hidden_sz)\n",
    "ht, (hn, cn) = lstm(arr)\n",
    "ht.shape #shape should be batch_sz x seq_len x hidden_sz = 5x10x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "3qCTcuxE5rSS",
    "outputId": "1d89fc72-3a04-4673-a1cc-d2c865cb64bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.4673 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 2.0708 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.9501 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.8558 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.8036 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.7528 ||: 100%|██████████| 38/38 [00:07<00:00,  4.91it/s]\n",
      "loss: 1.7202 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.6866 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.6657 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.6422 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.6272 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.6103 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.5986 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5860 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.5762 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5658 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.5583 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5513 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.5437 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5376 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.5309 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5283 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.5203 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5194 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.5110 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5109 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.5029 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.5024 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.4957 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4958 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4893 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4902 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4831 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4864 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4780 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4809 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4729 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4767 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4686 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4731 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.4644 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4685 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4603 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4663 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.4567 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4624 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4536 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4602 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.4508 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4576 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.4479 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4564 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4451 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4519 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4425 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4505 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4403 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4492 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4381 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4475 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4359 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4463 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4341 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4432 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4320 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4420 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4303 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4418 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4285 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4386 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.4266 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4372 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4251 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4366 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4236 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4342 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4222 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4348 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.4207 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4329 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.4197 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4306 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4181 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4309 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4170 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4296 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4158 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4284 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.4148 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4272 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4137 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4267 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4124 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4244 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.4118 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4244 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4106 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4253 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.4099 ||: 100%|██████████| 338/338 [03:42<00:00,  1.52it/s]\n",
      "loss: 1.4237 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4090 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4221 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.4078 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4218 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4071 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4219 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4062 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4217 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.4055 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4197 ||: 100%|██████████| 38/38 [00:07<00:00,  4.82it/s]\n",
      "loss: 1.4049 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4187 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.4042 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4193 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.4035 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4190 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4025 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4194 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.4020 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4176 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.4013 ||: 100%|██████████| 338/338 [03:42<00:00,  1.52it/s]\n",
      "loss: 1.4167 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.4006 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4163 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.4000 ||: 100%|██████████| 338/338 [03:42<00:00,  1.52it/s]\n",
      "loss: 1.4157 ||: 100%|██████████| 38/38 [00:07<00:00,  4.76it/s]\n",
      "loss: 1.3992 ||: 100%|██████████| 338/338 [03:43<00:00,  1.51it/s]\n",
      "loss: 1.4150 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3992 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4141 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3983 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4135 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3975 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4135 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3970 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4134 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3966 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4132 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3959 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4122 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3956 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4130 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3951 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4122 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3943 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4116 ||: 100%|██████████| 38/38 [00:07<00:00,  4.91it/s]\n",
      "loss: 1.3942 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4117 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3933 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4109 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3934 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4105 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3927 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4114 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3921 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4091 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3919 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4095 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3912 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4084 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3909 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4092 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3904 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4087 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3902 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4075 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3896 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4084 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3893 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4085 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3891 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4080 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3888 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4075 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3884 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4061 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3884 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4074 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3876 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4066 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3872 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4043 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3870 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4056 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3868 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4063 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3863 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4063 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3860 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4051 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3857 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4054 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3851 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4042 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3851 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4047 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3849 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4043 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3843 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3842 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4038 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3839 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4034 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3834 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4049 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3833 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4047 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3830 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4031 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3823 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4022 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3822 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4029 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3828 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4027 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3819 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4033 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3819 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4017 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3816 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4025 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3812 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4013 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.3806 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4017 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3807 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4021 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.3807 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4020 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3804 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4006 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3799 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4010 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3802 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4010 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3799 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4003 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3792 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4003 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3793 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4003 ||: 100%|██████████| 38/38 [00:07<00:00,  4.82it/s]\n",
      "loss: 1.3789 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4013 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3789 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3999 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3784 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.4005 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.3781 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3993 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3783 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3997 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3781 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3986 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3778 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3988 ||: 100%|██████████| 38/38 [00:07<00:00,  4.82it/s]\n",
      "loss: 1.3776 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3983 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3778 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3998 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3776 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3980 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3770 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3976 ||: 100%|██████████| 38/38 [00:07<00:00,  4.86it/s]\n",
      "loss: 1.3767 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3986 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.3766 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3996 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3767 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3976 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3763 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3987 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3758 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3984 ||: 100%|██████████| 38/38 [00:07<00:00,  4.88it/s]\n",
      "loss: 1.3760 ||: 100%|██████████| 338/338 [03:40<00:00,  1.53it/s]\n",
      "loss: 1.3986 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3757 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3976 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3758 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3975 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3754 ||: 100%|██████████| 338/338 [03:42<00:00,  1.52it/s]\n",
      "loss: 1.3987 ||: 100%|██████████| 38/38 [00:07<00:00,  4.87it/s]\n",
      "loss: 1.3752 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3973 ||: 100%|██████████| 38/38 [00:07<00:00,  4.85it/s]\n",
      "loss: 1.3757 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3981 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n",
      "loss: 1.3750 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3975 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3747 ||: 100%|██████████| 338/338 [03:42<00:00,  1.52it/s]\n",
      "loss: 1.3977 ||: 100%|██████████| 38/38 [00:07<00:00,  4.83it/s]\n",
      "loss: 1.3743 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3976 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3745 ||: 100%|██████████| 338/338 [03:41<00:00,  1.52it/s]\n",
      "loss: 1.3979 ||: 100%|██████████| 38/38 [00:07<00:00,  4.90it/s]\n",
      "loss: 1.3751 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.4012 ||: 100%|██████████| 38/38 [00:07<00:00,  4.89it/s]\n",
      "loss: 1.3751 ||: 100%|██████████| 338/338 [03:41<00:00,  1.53it/s]\n",
      "loss: 1.3980 ||: 100%|██████████| 38/38 [00:07<00:00,  4.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 141,\n",
       " 'peak_cpu_memory_MB': 2771.636,\n",
       " 'peak_gpu_0_memory_MB': 1115,\n",
       " 'training_duration': '9:25:44.141752',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 147,\n",
       " 'epoch': 147,\n",
       " 'training_loss': 1.3751222530060265,\n",
       " 'training_cpu_memory_MB': 2771.636,\n",
       " 'training_gpu_0_memory_MB': 1115,\n",
       " 'validation_loss': 1.401227615381542,\n",
       " 'best_validation_loss': 1.3973440904366343}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_mog = LanguageModel(MogLSTM(50, 125,5), vocab)\n",
    "mog_LSTM = train(lm_mog, N_EPOCHS, \"./run/mog2\")\n",
    "mog_LSTM.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ug9IzRBnX3oe"
   },
   "source": [
    "\n",
    "Based on our loss values and best epoch, it looks like our validation accuracy may be marginally better than the standard LSTM and it took about 60 fewer epochs to reach convergence. The Mogrifier LSTM makes no strong claims regarding increased speed (though they do claim lower complexity, which probably means they optimized much more than me), but they did indicate it acheives better accuracy. While promising, these results need to be tested with different datasets and neural networks to see how performance truly compares.\n",
    "\n",
    "The writers of the Mogrifier LSTM paper have stated they will release their code, but so far this has not ocurred. We can try a few tweaks (different randomization, different test model, other datasets, different test methods, more training) to better verify what's going on, but we'll leave it at this for now and look for the code release to see if there's anything else that can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reloading Models\n",
    "\n",
    "To reload models from checkpoints that are automatically saved (and either continue training or run inference) copy the following commands into a new cell and run (in this example we reload the first LSTM I created above). \n",
    "\n",
    "\n",
    "Note that your checkpoint may be automatically recognized and started from without needing to load it manually if it is located in the folder you are saving your training data to (in all the models above that's \"./run/modelName\").\n",
    "<pre><code>\n",
    "model2 = LanguageModel(NaiveLSTM(50, 125), vocab)\n",
    "with open(\"run/lstm/model_state_epoch_2.th\", 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSUQ-GkWlE1C"
   },
   "source": [
    "# Visualizing It\n",
    "\n",
    "Now that we've implemented a few things, lets go ahead and visualize it all. We're using tensorboard since it does a bunch of plotting work for us. The one downside at the moment is that AllenNLP's training class does not give us the option of comparing graphs from different models, so for now we'll look at them separately. \n",
    "\n",
    "**Note: This visualization requires tensorboard, which you may not have installed. It is intended for use on Google Colaboratory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_pg2sQYlE1C"
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "%tensorboard --logdir \"./run/mog2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If on Google Colaboratory you can save and export all the data we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ETILSLErur6_",
    "outputId": "41b66e4d-4648-4051-f284-9e7370c2689f"
   },
   "outputs": [],
   "source": [
    "#zip results\n",
    "!zip -r all.zip ./run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "zKuAXXpI2whz",
    "outputId": "26780ccf-55aa-4043-962c-d65fb3b93166"
   },
   "outputs": [],
   "source": [
    "#download/upload Colaboratory files to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjzGX8TR5UAt"
   },
   "outputs": [],
   "source": [
    "!cp all.zip /content/drive/My\\ Drive/all.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnguGfZMFlFo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lstm",
   "language": "python",
   "name": "venv_lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
